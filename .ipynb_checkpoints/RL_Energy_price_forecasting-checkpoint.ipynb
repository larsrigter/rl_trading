{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning model for forecasting spot prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Libaries & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\larsr\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime, time\n",
    "import warnings\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, model_from_json, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#Jupyter visualization\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 999\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#source: https://github.com/edwardhdlu/q-trader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, is_eval=False, model_name=\"\"):\n",
    "        self.state_size = state_size # normalized previous days\n",
    "        self.action_size = 3 # sit, buy, sell\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.inventory = []\n",
    "        self.model_name = model_name\n",
    "        self.is_eval = is_eval\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        if not is_eval:\n",
    "            self.model=self._model()\n",
    "        else:\n",
    "            self.model=load_seq_model(model_name)\n",
    "\n",
    "    def _model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=64, input_shape=(11,5), activation=\"relu\"))\n",
    "        model.add(keras.layers.Flatten(data_format=None))\n",
    "        model.add(Dense(units=32, activation=\"relu\"))\n",
    "        model.add(Dense(units=8, activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "    def act(self, state):\n",
    "        if not self.is_eval and random.random() <= self.epsilon:\n",
    "            randAct=random.randrange(self.action_size)\n",
    "#             if randAct!=0:\n",
    "#                 print(\"ran dom action \",randAct)\n",
    "            return randAct\n",
    "        options = self.model.predict(state)\n",
    "#         print(\"predicted by the model\",np.argmax(options[0]),options)\n",
    "        return np.argmax(options[0])\n",
    "\n",
    "    def expReplay(self, batch_size):\n",
    "        mini_batch = []\n",
    "        l = len(self.memory)\n",
    "        for i in range(l - batch_size + 1, l):\n",
    "            mini_batch.append(self.memory[i])\n",
    "\n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward\n",
    "#             print('reward',target)\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "#             print(target_f)\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay \n",
    "\n",
    "# prints formatted price\n",
    "def formatPrice(n):\n",
    "    return (\"-€\" if n < 0 else \"€\") + \"{0:.2f}\".format(abs(n))\n",
    "\n",
    "# returns the sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_m(x):\n",
    "    return 1 / (1 + np.exp(-1.0 * x))\n",
    "\n",
    "# returns an an n-day state representation ending at time t\n",
    "def getState(data, t, n):\n",
    "    d = t - n + 1\n",
    "    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0\n",
    "    res = []\n",
    "    for i in range(n - 1):\n",
    "        res.append(sigmoid_m(block[i + 1] - block[i]))\n",
    "    return np.array([res])\n",
    "\n",
    "def getState_m(data, t, n):\n",
    "    d = t - n + 1\n",
    "    if d>= 0:\n",
    "        block = data[d:t + 1]\n",
    "    else:\n",
    "        block = np.repeat(data[0:1], n, axis=0)\n",
    "        block[n-t:] = data[1:n+d]\n",
    "    res = []\n",
    "    for i in range(n - 1):\n",
    "        res.append(sigmoid_m(block[i + 1] - block[i]))\n",
    "#     print(\"shape of array\",np.array([res])[0].shape)\n",
    "    return np.array([res])\n",
    "\n",
    "def normalize(dataframe,ignore, scalarfile=None, save=True):\n",
    "    table=dataframe[:]\n",
    "    normalizer = MinMaxScaler()\n",
    "    cols_normalize=list(set(dataframe.keys())-set(ignore))\n",
    "    cols_renamed = [str(col)+ \"_n\" for col in cols_normalize]\n",
    "    table[cols_renamed] = table[cols_normalize]\n",
    "    if save:\n",
    "        normalizer.fit(table[cols_renamed])\n",
    "    else:\n",
    "        normalizer = joblib.load(\"scalars/\"+scalarfile+\".save\") \n",
    "    norm_df = pd.DataFrame(normalizer.transform(table[cols_renamed]), \n",
    "                                 columns=cols_renamed, \n",
    "                                 index=table.index)\n",
    "    join_df = table[table.columns.difference(cols_renamed)].join(norm_df)\n",
    "    table = join_df.reindex(columns = table.columns)\n",
    "    if save:\n",
    "        joblib.dump(normalizer, \"scalars/\"+scalarfile+\".save\")\n",
    "    return table, scalarfile, cols_renamed\n",
    "\n",
    "def save_model(model, filename):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"models/\"+filename+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"models/\"+filename+\".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "def load_seq_model(filename):\n",
    "    # load json and create model\n",
    "    json_file = open(\"models/\"+filename+\".json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"models/\"+filename+\".h5\")\n",
    "    print('`load_seq_model` > model and weights loaded : ', filename)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source APX spot prices: https://transparency.entsoe.eu/dashboard/show\n",
    "#source generation/consumption: https://data.open-power-system-data.org/time_series/\n",
    "\n",
    "key = 'Day_Ahead_Prices_NL'\n",
    "data = pd.read_csv(\"data/\" + key + \".csv\", delimiter=';')\n",
    "data['PRICE']=pd.to_numeric(data['PRICE'], errors='coerce')\n",
    "data['PRICE']= data['PRICE'].interpolate()\n",
    "data['WIND_OFFSHORE']= data['WIND_OFFSHORE'].interpolate()\n",
    "data['WIND_ONSHORE']= data['WIND_ONSHORE'].interpolate()\n",
    "data_train = data[100000:104001] # 1-1-2016 <> 12-11-2017 \n",
    "data_test = data[104001:108817] # 12-12-2017 <> 31-05-2018 \n",
    "data_train_n, scalarfile, cols_n = normalize(data_train, ['TIMESTAMP', 'DATE', 'LOAD_1'],\"MMS_\"+datetime.today().strftime('%Y%m%d'), save=True)\n",
    "data_test_n, scalarfile, cols_n = normalize(data_test, ['TIMESTAMP', 'DATE', 'LOAD_1'],\"MMS_20190125\", save=False)\n",
    "#data = list(data['PRICE']) #OLD 1D VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10\n",
      "0 4000\n",
      "100 4000\n",
      "200 4000\n",
      "300 4000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d3348b7f86bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtotal_profits\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mtotal_profits\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#logtype 0: log end result, 1: log intermediate result, 2: log actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;31m# total_profits = run_model(data_train_n, 11, 1, 64, logtype=0, training=False, model_name='RL_20190207')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-d3348b7f86bd>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(data, window_size, episode_count, batch_size, logtype, training, model_name)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpReplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlogtype\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-33caa5f7b930>\u001b[0m in \u001b[0;36mexpReplay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m#             print(target_f)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def run_model(data, window_size, episode_count, batch_size, logtype=0, training=True, model_name=None):\n",
    "    K.clear_session()\n",
    "\n",
    "    data_m = np.array(np.asmatrix(data[['PRICE_n', 'LOAD_2_n', 'SOLAR', 'WIND_ONSHORE', 'WIND_OFFSHORE']]))\n",
    "    is_eval = not training\n",
    "    agent = Agent(window_size, is_eval, model_name=model_name) \n",
    "    l = len(data_m) - 1\n",
    "    total_profits=[]\n",
    "    for e in range(episode_count):\n",
    "        print(\"Episode \" + str(e+1) + \"/\" + str(episode_count))\n",
    "        state = getState_m(data_m, 0, window_size + 1)\n",
    "        total_profit = 0\n",
    "        agent.inventory = []\n",
    "        for t in range(l):\n",
    "            if t%100==0:\n",
    "                print(t,l)\n",
    "            action = agent.act(state)\n",
    "            next_state = getState_m(data_m, t + 1, window_size + 1)\n",
    "            reward = 0\n",
    "            if action == 1: # buy\n",
    "                agent.inventory.append(data['PRICE'].iloc[t])\n",
    "                if logtype==2:\n",
    "                    print(\"Buy: \" + formatPrice(data['PRICE'].iloc[t]))\n",
    "            elif action == 2 and len(agent.inventory) > 0: # sell\n",
    "                bought_price = agent.inventory.pop(0)\n",
    "                reward = max(data['PRICE'].iloc[t] - bought_price, 0)\n",
    "                total_profit += data['PRICE'].iloc[t] - bought_price\n",
    "                if logtype>=2:\n",
    "                    print(\"Sell: \" + formatPrice(data['PRICE'].iloc[t]) + \" | Profit: \" + formatPrice(data['PRICE'].iloc[t] - bought_price))\n",
    "            done = True if t == l - 1 else False\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if training and (len(agent.memory) > batch_size):\n",
    "                agent.expReplay(batch_size)\n",
    "            if done:\n",
    "                if logtype>=1:\n",
    "                    print(\"--------------------------------\")\n",
    "                    print(\"Total Profit: \" + formatPrice(total_profit))\n",
    "                    print(\"--------------------------------\")\n",
    "                total_profits.append(total_profit)\n",
    "    if training:\n",
    "        save_model(agent.model, \"RL_\"+datetime.today().strftime('%Y%m%d'))\n",
    "    print('Total profit after ', episode_count, ' episodes: ', np.sum(total_profits)/episode_count)\n",
    "    return total_profits\n",
    "\n",
    "total_profits =  run_model(data_train_n, 11, 10, 32, logtype=0, training=True)\n",
    "newDf=pd.DataFrame(total_profits)\n",
    "newDf.to_csv(\"total profits \")\n",
    "#logtype 0: log end result, 1: log intermediate result, 2: log actions\n",
    "# total_profits = run_model(data_train_n, 11, 1, 64, logtype=0, training=False, model_name='RL_20190207')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vizualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 6), dpi=80)\n",
    "# ax = plt.axes()\n",
    "# ax2 = ax.twinx() \n",
    "\n",
    "# t_min = 0\n",
    "# t_max = 1000\n",
    "\n",
    "# data_v = data_train\n",
    "\n",
    "# ax.plot(data_v.index[t_min:t_max], data_v.PRICE[t_min:t_max], color='blue')\n",
    "# ax2.plot(data_v.index[t_min:t_max], data_v.LOAD_2[t_min:t_max], color='grey')\n",
    "# ax2.plot(data_v.index[t_min:t_max], data_v.SOLAR[t_min:t_max], color='yellow')\n",
    "# ax2.plot(data_v.index[t_min:t_max], data_v.WIND_ONSHORE[t_min:t_max], color='red')\n",
    "# ax2.plot(data_v.index[t_min:t_max], data_v.WIND_OFFSHORE[t_min:t_max], color='#FFAA55')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
